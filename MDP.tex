Markov Decision Process
\df{Controlled Markov chains:} $\mathcal{T} = \{0, \ldots, T-1\}$ (finite horizon) or infinite horizon, $\mathcal{S}$ finite state space, where $S_t \subseteq \mathcal{S}$, $\mathcal{A}$ finite action space where $A_t(s) \subseteq \mathcal{A}$, $s \in S_t$, $\mathcal{P}_t(\cdot|s,a)$ transition probability from state $s$ to state $s'$ given action $a$ (distribution) that is $\mathbb{P}(s_{t+1} = s' | s_t = s, a_t = a) = \mathcal{P}_t(s'|s,a)$. States $s' \in S_{t+1}$.

\df{The Induced Stochastic Process:}
Let $\mathcal{P}_0(s_0)$ be the initial state distribution, $\pi \in \Pi_{\mathcal{H}_S}$, they induce a probability distribution over any finite state-action sequence $\mathcal{H}_T = (s_0, a_0, \ldots, s_T)$, given by
\[\mathbb{P}(\mathcal{H}_t) = \mathcal{P}_0(s_0) \prod_{t=0}^{T-1} \mathcal{P}_t(s_{t+1}|s_t, a_t)\pi(a_t|\mathcal{H}_t),\]
where $\mathcal{H}_t = (s_0, a_0, \ldots, s_t)$.

\df{Markov policy induces a Markov chain:}
$\mathbb{P}(s_{t+1} = s' | s_t = s) = \sum_{a} \mathcal{P}_t(s'|s,a) \cdot \pi_t(a|s).$

\df{Finite horizon:}
\begin{align*}
\mathcal{V}_T^\pi(s) &= \mathbb{E}^\pi\left[\sum_{t=0}^{T} R_t | s_0 = s\right] \equiv \mathbb{E}^{\pi  ,s}\left[\sum_{t=0}^{T} R_t\right] \\ &= \sum_{t=0}^{T-1} \mathbb{E}_\pi[r_t(s_t,a_t) | s_0 = s] + \mathbb{E}_\pi[r_T(s_T) | s_0 = s]
\end{align*}


\df{Infinite horizon:} $ \mathcal{V}_\gamma^\pi(s)  = \sum_{t=0}^\infty \mathbb{E}_\pi[\gamma^t r_t(s_t,a_t) | s_0 = s]$.
\begin{clm} if $r(s, a) \in [0,1]$ then $0 \leq V^{\pi}_{\gamma}(s) \leq \frac{1}{1-\gamma}$ and the sum of rewards after $t \geq \log_{\gamma} \varepsilon$ contribute at most $\frac{\varepsilon}{1-\gamma}$\end{clm}
\df{Extended rewards:} $R_t = \tilde{r}_t(s_t, a_t, s_{t+1})$. $r_t(s, a) = \mathbb{E}[R_t|s_t = s, a_t = a] = \sum p(s_{t+1}|s_t, a_t) \cdot s_{t+1}\tilde{r}_t(s_t, a_t, s_{t+1})$
\begin{lem}{\textbf{(finite horizon policy evaluation):}}\\let $\pi = (\pi_0, \dots , \pi_{T-1}) \in \Pi^{MD}$. \\Define $V^{\pi}_k(s) = \mathbb{E}^{\pi}[\sum_{t=k}^{T} R_t|s_k = s]$ ($V^{\pi}_0(s) = V^{\pi}(s))$ then $V^{\pi}_k(s) = r_k(s, \pi_k(s)) + \sum p_k(s'|s, \pi_k(s))V^{\pi}_{k+1} (s')$.\\ $s'\in S_{k+1}$, $\forall s \in S_k$ for $k = T - 1, \dots ,0$ starting with $V^{\pi}_T(s) = r_T(s)$\end{lem}

\begin{clm} For any policy $\pi$, $|V_\gamma^\pi(s)| \leq \frac{R_{\text{Max}}}{1-\gamma}$, where $R_{\text{Max}} \geq |r(s_t,a_t)|$\end{clm}


\begin{lemm} For a fixed stationary policy $\pi$, the value function $\mathcal{V}_\pi$ satisfies the following set of $|S|$ linear equations:
\begin{align*}
\mathcal{V}_\pi(s) = r(s, \pi(s)) + \gamma \sum_{s'} \mathcal{P}_\pi(s'|s) \mathcal{V}_\pi(s') \text{ for $^1$ }\end{align*}
\end{lemm}

\begin{greyboxedalgorithm}
\vspace{-0.5cm}
\begin{algorithm}[H]
\caption{Finite-Horizon MDP (value iteration)}
\begin{footnotesize}
\begin{algorithmic}[1]
\STATE Set $V_T(s) = r_T(s)$ for $s \in S_T$.
\STATE For $k = T - 1, \ldots, 0$ Compute $V_k(s)$ using the following recursion:
   \begin{align*} V_k(s) =& \max_{a \in A_k} \ r_k(s, a) \\&+ \sum_{s' \in S_{k+1}} p_k(s' \mid s, a) V_{k+1}^\pi(s')  \end{align*} 
 Where $\quad s \in S_k$. We have $V_k(s) = V_k^*(s)$.
\STATE Optimal policy: Any Markov policy $\pi^*$ that satisfies, for $t = 0, \ldots, T-1$  \begin{align*}\pi_t^*(s) \in \arg &\max_{a \in A_k} r_k(s, a) \\&+ \sum_{s' \in S_{k+1}} p_k(s' \mid s, a) V_{k+1}^\pi(s')  \end{align*}  for all $s \in S_t$, is an optimal policy. Furthermore, $\pi^*$ maximizes $V_\pi(s_0)$ simultaneously for every initial state $s_0 \in S_0$.
\end{algorithmic}
\end{footnotesize}

\end{algorithm}
\end{greyboxedalgorithm}

\begin{deff}
\df{The $Q$ function:}
\begin{align*}&Q^\pi(s, a) = \sum_{s'} p(s' \mid s, a) \left[ r(s, a, s') + \gamma V^\pi(s') \right]\\
&Q_k^*(s, a) = r_k(s, a) + \sum_{s'} p_k(s' \mid s, a) V_{k+1}^*(s').
\end{align*}

\end{deff}

\begin{clm}
$V^\pi(s) = \sum_{a \in A} \pi(a \mid s) Q^\pi(s, a)$ for all $s \in S$.
\end{clm}

\begin{clm}$\V^*(s) = \max_a Q^*(s, a)$, $\pi_k^*(s) \in \arg \max_{a \in A_k} Q_k^*(s, a)$.
\end{clm}

\begin{deff}
\dfi{Infinite horizon problems:} same setting as before (now $\mathbb{E}[R_t] = r(s_t, a_T)$) with discounted return:
\begin{align*}
V^\pi(s) &= \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 = s \right] \\ &\equiv \mathbb{E}_{\pi, s} \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \right]\text{for $ \gamma \in (0, 1)$.}
\end{align*}
\end{deff}


\begin{clm}
For any policy $\pi$, $|V^\pi(s)| \leq \frac{R_{\text{Max}}}{1 - \gamma}$, where $R_{\text{Max}} \geq |r(s, a)|$.
\end{clm}

\begin{lem}
For $\pi \in \Pi_S^D$, the value function $V^\pi$ satisfies the following set of $|S|$ linear equations:
\begin{align*}
V^\pi(s) = r(s, \pi(s)) + \gamma \sum_{s'} p(s' \mid s, \pi(s)) V^\pi(s'), \\ \text{or in vector form: } V^\pi = r_\pi + \gamma P_\pi V^\pi.
\end{align*}

\end{lem}

\begin{lem}
The set of linear equations in the lemma above has a unique solution $V^\pi$, which is given by
\[
V^\pi = (I - \gamma P_\pi)^{-1} r_\pi.
\]
\end{lem}
\begin{greyboxedalgorithm}
\vspace{-0.5cm}
\begin{algorithm}[H]
\begin{footnotesize}
\caption{Discounted Policy Eval}
\begin{algorithmic}[1]
\STATE Let $\V_0 = (\V_0(s))_{s \in S}$ be arbitrary.
\STATE\textbf{for} $n = 0, 1, \ldots$
\STATE \ \ $\V_{n+1}(s) = r(s, \pi(s)) +$\\$ \gamma \sum_{s'} p(s' \mid s, \pi(s)) \V_n(s')$ for all $s \in S$. \\or equivalently $\V_{n+1}=r^\pi +\gamma P \V_n$
\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\end{greyboxedalgorithm}
\prop  We have $
\lim_{{n\to\infty}} \frac{{\mathcal{V}_n(s)}}{{\mathcal{V}(s)}} = \mathcal{V}_\pi(s) \quad \forall s \in \mathcal{S}.$
\begin{thm}[Bellman Optimality Equation]
For $V^\gamma(s) = \sup_{\pi \in \Pi_{H,S}} V^\pi(s)$:
    {\color{blue}(1)} $V^*$ is the unique solution of the following set of (nonlinear) equations:
    \[
    V(s) = \max_a \left\{ r(s, a) + \gamma \sum_{s'} p(s' \mid s, a) V(s') \right\}.\footnote{ forall $s \in S$}
    \]
    {\color{blue}(2)} Any stationary policy $\pi^*$ that satisfies $$\pi^*(s) \in \arg \max_{a \in A} \left\{ r(s, a) + \gamma \sum_{s'} p(s' \mid s, a) V(s') \right\}$$ is an optimal policy (for any initial state $s_0 \in S$).
\end{thm}


\begin{deff}
For a fixed stationary policy $\pi: S \rightarrow A$, define fixed policy DP operator $T_\pi: \mathbb{R}^{|S|} \rightarrow \mathbb{R}^{|S|}$ as follows: for all $V = (V(s)) \in \mathbb{R}^{|S|}, \forall s \in S, (T_\pi(V))(s) = r(s, \pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V(s')$. In our column-vector notation: $T_\pi(V) = r_\pi + \gamma P_\pi V$.
\end{deff}


\begin{thm}
$T \in \{T^*, T_\pi\}$ is a $\gamma$-contraction operator with respect to the max-norm, namely $\|T(\mathcal{V}_1) - T(\mathcal{V}_2)\|_\infty \leq \gamma\|\mathcal{V}_1 - \mathcal{V}_2\|_\infty$.
\end{thm}

\begin{algorithm}
(Value Iteration)
\begin{enumerate}
\item Let $\mathcal{V}_0 = (\mathcal{V}_0(s))_{s \in S}$ be arbitrary. 
\item For $n = 0,1, \ldots$ set $\mathcal{V}_{n+1}(s) = \max_{a \in A} \{r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a)\mathcal{V}_n(s')\}$ $\forall s \in S$.
\item Apply a stopping rule to obtain a required accuracy (In terms of DP operator $T^*$, VI is simply $\mathcal{V}_{n+1} = T^*(\mathcal{V}_n)$).
\end{enumerate}
\end{algorithm}

\begin{lem}
If $\|\mathcal{V}_{n+1} - \mathcal{V}_n\|_\infty < \frac{\epsilon(1-\gamma)}{2\gamma}$ then $\|\mathcal{V}_{n+1} - \mathcal{V}^*\|_\infty < \frac{\epsilon}{2}$ and $|\mathcal{V}_{\pi_{n+1}} - \mathcal{V}^*| \leq \epsilon$ where $\pi_{n+1}$ is the greedy policy w.r.t $\mathcal{V}_{n+1}$.
\end{lem}

\begin{clm}
$T_\pi(\mathcal{V}_\pi) = \mathcal{V}_\pi$, $T_{\pi_n}(\mathcal{V}_n) = T^*(\mathcal{V}_n)$, $T_{\pi_n}(\mathcal{V}_{\pi_n}) \leq T^*(\mathcal{V}_{\pi_n})$, $T_{\pi_{n+1}}(\mathcal{V}_{\pi_n}) = T^*(\mathcal{V}_{\pi_n})$ ($\pi_n , \pi_{n+1}$ greedy w.r.t $\mathcal{V}_n,\mathcal{V}_{n+1}$).
\end{clm}

\begin{thm}
We have $\lim_{n \to \infty} \mathcal{V}_n = \mathcal{V}^*$. The rate of convergence is exponential at rate $\mathcal{O}(\gamma^n)$.
\end{thm}
\begin{greyboxedalgorithm}
\vspace{-0.5cm}
\begin{algorithm}[H]
\begin{footnotesize}
\caption{Policy Iteration}
\begin{algorithmic}[1]
\STATE choose some stationary policy $\pi_0$.
\STATE Policy Evaluation: computee $\mathcal{V}_{\pi_k}$.
\STATE Policy Improvement: compute $\pi_{k+1}$: \begin{align*}\pi_{k+1}(s) \in &\arg\max_{a \in A} \{r(s, a) \\&+ \gamma \sum_{s' \in S} p(s'|s, a)\mathcal{V}_{\pi_k}(s')\}
\end{align*}
\STATE Stop if $\pi_{k+1} = \pi_k$ (or if $\mathcal{V}_{\pi_k}$ satisfied the optimality equation), else continue.
\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\end{greyboxedalgorithm}
\begin{thm}
The following statements hold: (1)$\mathcal{V}_{\pi_{k+1}} \geq \mathcal{V}_{\pi_k}$, (2) $\mathcal{V}_{\pi_{k+1}} = \mathcal{V}_{\pi_k}$ iff $\pi_k$ is an optimal policy (3)$\pi_k$ converges after a finite number of steps since the number of stationary policies is finite.
\end{thm}

\begin{thm}
Let $\{V_{I_n}\}$ be the sequence of values created by the VI algorithm (where $V_{I_{n+1}} = T^*(V_n)$) and $\{P_{I_n}\}$ be the sequence of values created by PI algorithm, i.e., $P_{I_n} = \ldots$
\end{thm}
